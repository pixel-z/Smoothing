{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WoSMNFNvxCok"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from collections import defaultdict\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya8dQBuTIxSP",
        "outputId": "956d3fe0-9647-4fc2-c3c7-11796c689bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKyZsKTgcVIR",
        "outputId": "a7970a5f-003b-4658-8b73-de72810a6d9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Bumping dj sefs mixtape nowww this is my music new skooooool\\n',\n",
              " '#ieroween THE STORY OF IEROWEEN! THE VIDEO ->>>>>>>>>>>>>>>>>>>>>> http://bit.ly/2VFPAV <<<< JUST FOR FRANK !!! Ã§\\n',\n",
              " 'trick or treating at the mall today; ZOO! last year we had left-overs, this year we ran out!\\n',\n",
              " \"@Ussk81 PMSL!!! I try not to stare but I can't help it, like compulsive viewing!!\\n\",\n",
              " '@Sc0rpi0n676 btw - is there a remote chance i will see you later?\\n']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file = \"./drive/MyDrive/NLP/general-tweets.txt\"\n",
        "with open(file) as f:\n",
        "  twitter_corpus = f.readlines()\n",
        "  # len(twitter_corpus)\n",
        "twitter_corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O7wOAT2Trkt"
      },
      "outputs": [],
      "source": [
        "def Tokenizer(st = None):\n",
        "    word_regex=r\"#\\S+|@\\S+|Dr\\.|Mr\\.|Mrs\\.|[0-9]+\\.[0-9]+|\\w*'t|\\w*'s|\\w+|'|,|\\.|\\w*'t\\b|\\(|\\)|\\\\|\\/|-|!\\s+\"\n",
        "    url_regex = r\"https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}\"\n",
        "    word_regex = url_regex + r\"|\" + word_regex\n",
        "    \n",
        "    hash_regex = r\"#\\S+\"\n",
        "    mention_regex = r\"@\\S+\"\n",
        "    \n",
        "    inp = \"\"\"\n",
        "    #ieroween THE STORY OF IEROWEEN! THE\n",
        "    VIDEO -»»»»»»»»»»» http://bit.ly/2VFPAV ««\n",
        "    JUST FOR FRANK !!! ÃƒÂ§\n",
        "    \"\"\"\n",
        "    if st != None:\n",
        "        inp = st\n",
        "    ret = re.findall(word_regex, inp)\n",
        "    ret = [re.sub(url_regex, \"<URL>\", x) for x in ret]\n",
        "    ret = [re.sub(hash_regex, \"<HASHTAG>\", x) for x in ret]\n",
        "    ret = [re.sub(mention_regex, \"<MENTION>\", x) for x in ret]\n",
        "    return [s.strip() for s in ret]\n",
        "\n",
        "# def Tokenizer(st = None):\n",
        "#   url_regex = r\"https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}\"\n",
        "\n",
        "#   sent=re.sub(url_regex,'<URL>',st)\n",
        "#   sent=re.sub(\"#(\\w+)\",\"<HASTAG>\",sent)\n",
        "#   sent=re.sub(\"@(\\w+)\",\"<MENTION>\",sent)\n",
        "#   string=\"#@$%><*&/?.!~`-_=|.:;,\"\n",
        "#   for i in string:\n",
        "#     reg='\\\\'\n",
        "#     reg+=i\n",
        "#     reg+=' + '\n",
        "#     sent=re.sub(reg,i,sent)\n",
        "#   for i in string:\n",
        "#     reg='\\\\'\n",
        "#     reg+=i\n",
        "#     reg+='+'\n",
        "#     sent=re.sub(reg,i,sent)\n",
        "#   # print(sent)\n",
        "#   # line=re.findall(word_regex,sent)\n",
        "#   return sent.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xQ-BiUkunaF",
        "outputId": "9f0a1934-76c6-494d-d0d7-16f17dd44883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#ieroween THE STORY OF IEROWEEN! THE VIDEO ->>>>>>>>>>>>>>>>>>>>>> http://bit.ly/2VFPAV <<<< JUST FOR FRANK !!! Ã§\n",
            "\n",
            "['<HASHTAG>', 'THE', 'STORY', 'OF', 'IEROWEEN', '!', 'THE', 'VIDEO', '-', '<URL>', 'JUST', 'FOR', 'FRANK', '!', 'Ã']\n"
          ]
        }
      ],
      "source": [
        "# Checking on twitter corpus\n",
        "check = Tokenizer(twitter_corpus[1])\n",
        "print(twitter_corpus[1])\n",
        "print(check)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "RZzPcWaOypfU"
      },
      "outputs": [],
      "source": [
        "# Storing tokenized output of general-tweets into 2019111031_tokenize.txt\n",
        "file = \"./drive/MyDrive/NLP/general-tweets.txt\"\n",
        "with open(file) as f:\n",
        "  data = f.readlines()\n",
        "\n",
        "out = \"\"\n",
        "for i, line in enumerate(data):\n",
        "    data[i] = \" \".join(Tokenizer(line))\n",
        "    # print(data[i])\n",
        "    out += (data[i]+\"\\n\")\n",
        "    \n",
        "file1 = open(\"2019111031_tokenize.txt\",\"w\")\n",
        "file1.write(out)\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0wzaLXdZyng"
      },
      "outputs": [],
      "source": [
        "# class stores info about the corpus\n",
        "class Corpus:\n",
        "  def __init__(self, corpus):\n",
        "    self.total_words = 0\n",
        "    self.test_sentences = []\n",
        "    self.train_sentences = []\n",
        "    self.test_indexes = random.sample(range(len(corpus)), 1000)\n",
        "    self.corpus = corpus\n",
        "    self.unique_history_count = defaultdict(lambda: 0)  # same as a {}, except defaultdict initializes with 0\n",
        "    self.gramCount = defaultdict(lambda: 0)             # map mp[I,am,a,man] = c\n",
        "\n",
        "  # tokenize and store info about corpus\n",
        "  def process(self):\n",
        "    for i, line in enumerate(self.corpus):\n",
        "      if i in self.test_indexes:\n",
        "        self.test_sentences += [line]\n",
        "        continue\n",
        "      \n",
        "      self.train_sentences += [line]\n",
        "      \n",
        "      words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "      words += Tokenizer(line)\n",
        "      words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "      \n",
        "      if len(words) <= 6:\n",
        "        continue\n",
        "      # print(words)\n",
        "      # break\n",
        "      \n",
        "      size = len(words)\n",
        "      self.total_words += size\n",
        "      for j, word in enumerate(words):\n",
        "        # 4-gram cnts\n",
        "        if j < size - 3:\n",
        "          if (words[j], words[j+1], words[j+2], words[j+3]) not in self.gramCount:\n",
        "            self.unique_history_count[(words[j], words[j+1], words[j+2], words[j+3])] += 1\n",
        "          self.gramCount[(words[j], words[j+1], words[j+2], words[j+3])] += 1\n",
        "\n",
        "        # Tri-gram\n",
        "        if j < size - 2:\n",
        "          if (words[j], words[j+1], words[j+2]) not in self.gramCount:\n",
        "            self.unique_history_count[(words[j], words[j+1], words[j+2])] += 1\n",
        "          self.gramCount[(words[j], words[j+1], words[j+2])] += 1\n",
        "\n",
        "        # Bi-gram\n",
        "        if j < size - 1:\n",
        "          if (words[j], words[j+1]) not in self.gramCount:\n",
        "            self.unique_history_count[(words[j], words[j+1])] += 1\n",
        "            if (words[j],) not in self.unique_history_count:\n",
        "              self.unique_history_count[(words[j], )] += 1\n",
        "          self.gramCount[(words[j], words[j+1])] += 1\n",
        "\n",
        "        # Uni-gram\n",
        "        if (words[j],) not in self.gramCount:\n",
        "            self.unique_history_count[()] += 1\n",
        "        self.gramCount[(words[j],)] += 1\n",
        "        \n",
        "    self.gramCount[()] = self.total_words\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mI56ybXC-qBC"
      },
      "outputs": [],
      "source": [
        "# Returns probability of the sequence \n",
        "def KneserNey(corpus, sentence, D=0.75):\n",
        "  # corpus = corpus class object\n",
        "  \n",
        "  ans=0.0\n",
        "  if(len(sentence)==1):\n",
        "      if sentence in corpus.gramCount:\n",
        "          ans=max(corpus.gramCount[sentence]-D, 0)/corpus.gramCount[()]\n",
        "      ans+=D/(corpus.gramCount[()])\n",
        "      return ans\n",
        "  if sentence in corpus.gramCount:\n",
        "      ans=ans+max(corpus.gramCount[sentence]-D, 0)/corpus.gramCount[sentence[:-1]]\n",
        "      return ans+D*KneserNey(corpus,sentence[1:])*corpus.unique_history_count[sentence[:-1]]/corpus.gramCount[sentence[:-1]]\n",
        "  if sentence[:-1] in corpus.gramCount and sentence[1:] in corpus.gramCount:\n",
        "      return ans+D*KneserNey(corpus,sentence[1:])*corpus.unique_history_count[sentence[:-1]]/corpus.gramCount[sentence[:-1]]\n",
        "  else:\n",
        "      return D/(corpus.gramCount[()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-an36DmLE-l"
      },
      "outputs": [],
      "source": [
        "def Wittenbell(corpus_class, sentence):\n",
        "    ans=0.0\n",
        "    \n",
        "    if(len(sentence)==0):\n",
        "        return 1/(corpus_class.gramCount[()]+corpus_class.unique_history_count[()])\n",
        "    if(len(sentence)==1):\n",
        "        return corpus_class.gramCount[(sentence[-1],)]/(corpus_class.gramCount[()]+corpus_class.unique_history_count[()]) + 1/(corpus_class.gramCount[()]+corpus_class.unique_history_count[()])\n",
        "    if sentence in corpus_class.gramCount:\n",
        "        ans=0.0\n",
        "        ans=ans+((corpus_class.gramCount[sentence]+corpus_class.unique_history_count[sentence[:-1]]*Wittenbell(corpus_class,sentence[1:]))/(corpus_class.gramCount[sentence[:-1]]+corpus_class.unique_history_count[sentence[:-1]]))\n",
        "        return ans\n",
        "    if sentence[1:] and sentence[:-1] in corpus_class.gramCount:\n",
        "        ans=((corpus_class.unique_history_count[sentence[:-1]]*Wittenbell(corpus_class,sentence[1:]))/(corpus_class.gramCount[sentence[:-1]]+corpus_class.unique_history_count[sentence[:-1]]))\n",
        "        return ans\n",
        "    if (sentence[-1],) in corpus_class.gramCount:\n",
        "        return corpus_class.gramCount[(sentence[-1],)]/(corpus_class.gramCount[()]+corpus_class.unique_history_count[()]) + 1/(corpus_class.gramCount[()]+corpus_class.unique_history_count[()])\n",
        "    return 1/(corpus_class.gramCount[()]+corpus_class.unique_history_count[()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNFJ7HX5RXdI",
        "outputId": "7514d97c-83e7-4b69-8b8e-99b50f0738fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 Please rise, then, for this minute' s silence.\n",
            "\n",
            "100 Hence also this joint debate involving transport and environmental management Members.\n",
            "\n",
            "200 In the field of immigration, borders and asylum, the Commission presented a proposal for a regulation on setting up the Eurodac database, and a proposal for a directive on conditions for admitting third country nationals into Member States of the European Union with a view to reuniting families, as part of a vigorous policy of integrating legal residents in the territory of the European Union.\n",
            "\n",
            "300 Mr President, Commissioner, I would firstly like to highlight the excellent work carried out by Mrs Berger and the Committee on Legal Affairs and the Internal Market generally, which has incorporated innovative ideas into this directive, which I hope will be approved by the Commission and the Council.\n",
            "\n",
            "400 It should prevent society from splintering.\n",
            "\n",
            "500 Unless this CAP is overthrown, no programme can ensure the survival of small and medium-scale farmers and the social and economic regeneration of the countryside.\n",
            "\n",
            "600 The Commission' s communication recommends that experience should be more widely shared, a high-level group formed and benchmarking introduced.\n",
            "\n",
            "700 We are therefore taking advantage of this political will in an attempt to define the outlines of a compromise, and we have already begun work on this matter.\n",
            "\n",
            "800 Back in 1994, the Joint Assembly passed an initial resolution, in which our fellow Member Dominique Souchet was closely involved, expressing its alarm at the foreseeable consequences of the Uruguay Round, which contradicted the very principle of Community preference which was fundamental to this cooperation.\n",
            "\n",
            "900 The Minutes of this sitting will be submitted for approval by Parliament at the beginning of the next part-session.\n",
            "\n",
            "Testing sentences done\n",
            "0 Resumption of the session\n",
            "\n",
            "2000 As an elected representative for the Loire-Atlantique region, unfortunately, I can testify to this.\n",
            "\n",
            "4000 What can the European Union do in these circumstances?\n",
            "\n",
            "6000 For this reason, the draft reform, which has been drawn up by the rapporteur, Vice-President Marinho, is to be welcomed.\n",
            "\n",
            "8000 For example, I know of one Member State that would like to use a large proportion of the entire programme for one component, namely refugees.\n",
            "\n",
            "10000 The legislation should also have written into it the fact that the international conventions, such as the OSPAR Convention must be respected.\n",
            "\n",
            "12000 Because the joint motion for a resolution proposes a range of effective mechanisms for ensuring coherence, I shall use the very short time allotted to me here just to mention one more such mechanism, namely obliging the Commission to carry out an evaluation of new, relevant legislation from this aspect.\n",
            "\n",
            "14000 The report tackles stability in the region in a structural manner. This is also the purpose of the pending agreement with the Republic of Macedonia which we are debating today.\n",
            "\n",
            "16000 It is a vital precondition for strengthening democratic institutions and civil society.\n",
            "\n",
            "18000 Mr President, Commissioner, the reason why I was so pleased with your intervention was that you had the courage to list the Member States where the political representation of women is a success, such as, for example, in Finland, France and Sweden, and equally, you had the courage to list the Member States where things in this respect are not going so well, such as, for example, my own country and Greece too, which you forgot to mention.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file = \"./drive/MyDrive/NLP/europarl-corpus.txt\"\n",
        "\n",
        "with open(file) as f:\n",
        "  europarl = f.readlines()\n",
        "\n",
        "europarl = Corpus(europarl)\n",
        "europarl.process()\n",
        "\n",
        "# europarl test_perplexities\n",
        "kneserney_perplexities = []\n",
        "testing_output=\"\"\n",
        "for j, line in enumerate(europarl.test_sentences):\n",
        "  words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "  words += Tokenizer(line)\n",
        "  words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "  prob = 0\n",
        "\n",
        "  if j%100 == 0:\n",
        "    print(j, line)\n",
        "\n",
        "  size = len(words)\n",
        "  for i, word in enumerate(words):\n",
        "    if i < size-3:\n",
        "      kn = KneserNey(europarl, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "      log_kn = math.log(kn)\n",
        "      prob += log_kn\n",
        "  final = math.e**(-prob/size)\n",
        "  testing_output=testing_output+((line.strip())+\"\\t\"+str(final))+\"\\n\"\n",
        "  kneserney_perplexities.append(final)\n",
        "\n",
        "testing_output = str((sum(kneserney_perplexities)/len(kneserney_perplexities))) + '\\n' + testing_output\n",
        "file1=open(\"2019111031-LM1-test-perplexity.txt\", \"w\")\n",
        "file1.write(testing_output)\n",
        "file1.close()\n",
        "\n",
        "print(\"Testing sentences done\")\n",
        "\n",
        "# europarl train_perplexities\n",
        "kneserney_perplexities = []\n",
        "training_output=\"\"\n",
        "for j, line in enumerate(europarl.train_sentences):\n",
        "  words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "  words += Tokenizer(line)\n",
        "  words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "  prob = 0\n",
        "\n",
        "  if j%2000 == 0:\n",
        "    print(j, line)\n",
        "\n",
        "  size = len(words)\n",
        "  for i, word in enumerate(words):\n",
        "    if i < size-3:\n",
        "      kn = KneserNey(europarl, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "      log_kn = math.log(kn)\n",
        "      prob += log_kn\n",
        "  final = math.e**(-prob/size)\n",
        "  training_output=training_output+((line.strip())+\"\\t\"+str(final))+\"\\n\"\n",
        "  kneserney_perplexities.append(final)\n",
        "\n",
        "training_output = str((sum(kneserney_perplexities)/len(kneserney_perplexities))) + '\\n' + training_output\n",
        "file1=open(\"2019111031-LM1-train-perplexity.txt\", \"w\")\n",
        "file1.write(training_output)\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUXL2HidzLvn",
        "outputId": "52f45441-b526-4c54-8c0d-d8b5d999ee1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 In this study we demonstrate with organoid cultures of fetal rat lungs that the reductive side of 11-hydroxysteroid dehydrogenase (11-HSD) regulates the intracellular activation of GCS.\n",
            "\n",
            "100 To avoid these disadvantages at high altitude a concept of living at moderately high altitude and training at lower elevations, termed \"live high - train low\" evolved, opposing the conventional concept of \"live high - train high\".\n",
            "\n",
            "200 The median survival time for patients with disseminated malignant melanoma is 5-7 months; it is still not really known whether therapeutic measures prolong survival.\n",
            "\n",
            "300 fixated metastases with a diameter greater than 6 cm, individually adapted therapy in necessary because of the extremely poor prognosis.\n",
            "\n",
            "400 The intensified therapy consisted of pravastatin 20-40mg with addition of colestyramine and/or nicotinic acid, if necessary, to achieve an LDL cholesterol <=130mg/dl.\n",
            "\n",
            "500 Six patients with granuloma faciale, including patients with multiple lesions, were treated successfully with cryosurgery.\n",
            "\n",
            "600 SF was obtained from 115 patients with disorders of the knee, including gonarthrosis (n = 44), meniscal tears (n = 10), rheumatoid arthritis (n = 53), and reactive arthritides (n = 8).\n",
            "\n",
            "700 A child with typical signs of the cat eye-syndrom except the characteristic feature of a cat eye is reported.\n",
            "\n",
            "800 Since the first dermatological outpatient clinic was started in Dresden in 1869, it has seen the practice of dermatology in the broadest sense, with scientific research activity and innovation.\n",
            "\n",
            "900 There were no postoperative complications.\n",
            "\n",
            "Testing sentences done\n",
            "0 Although there are a few isolated reports in the literature suggesting that sugar beet pollen is highly antigenic, hypersensitivity to components of sugar beet is not a common disease.\n",
            "\n",
            "2000 The present paper presents data obtained over a 12 year period, on the matrix synthesis and turnover in some 650 arthritic and 180 non-arthritic (N) human cartilages using a standardised in vitro method.\n",
            "\n",
            "4000 Whereas those rare forms of localized pleural mesotheliomy are being detected incidentally and can be cured by complete resection, most patients with diffuse malignant mesothelioma present with an advanced stage of disease.\n",
            "\n",
            "6000 Following non-operative treatment one child suffered a coxa vara and another child suffered a avascular femoral head necrosis in combination with coxa vara and leg length shortening of 4 cm.\n",
            "\n",
            "8000 545 male patients with a tentative diagnosis \"urethritis\" were examined between November 1984 and December 1994 in the Department of Dermatology and Venerology of the Military Hospital in Ulm.\n",
            "\n",
            "10000 Methods: We report a case of a 49-year-old patient with an intrauterine device.\n",
            "\n",
            "12000 With this strategy the patient has been asymptomtic during the eight month follow-up period.\n",
            "\n",
            "14000 However, the quantification of the increase in colour Doppler signals after Levovist in the cited study relied only on descriptive criteria defined by the investigator, resulting in a subjective scoring system.\n",
            "\n",
            "16000 An unusual shaped piece of skin was found in the oral cavity of a woman who had been stabbed to death.\n",
            "\n",
            "18000 The diagnostic arsenal of risk parameters comprised heart rate variability, baroreflex sensitivity, and more traditional markers such as non-sustained ventricular tachycardia, left ventricular ejection fraction, and ventricular late potentials.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file = \"./drive/MyDrive/NLP/medical-corpus.txt\"\n",
        "\n",
        "with open(file) as f:\n",
        "  medical = f.readlines()\n",
        "\n",
        "medical = Corpus(medical)\n",
        "medical.process()\n",
        "\n",
        "# medical test_perplexities\n",
        "kneserney_perplexities = []\n",
        "testing_output=\"\"\n",
        "for j, line in enumerate(medical.test_sentences):\n",
        "  words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "  words += Tokenizer(line)\n",
        "  words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "  prob = 0\n",
        "\n",
        "  if j%100 == 0:\n",
        "    print(j, line)\n",
        "\n",
        "  size = len(words)\n",
        "  for i, word in enumerate(words):\n",
        "    if i < size-3:\n",
        "      kn = KneserNey(medical, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "      log_kn = math.log(kn)\n",
        "      prob += log_kn\n",
        "  final = math.e**(-prob/size)\n",
        "  testing_output=testing_output+((line.strip())+\"\\t\"+str(final))+\"\\n\"\n",
        "  kneserney_perplexities.append(final)\n",
        "\n",
        "testing_output = str((sum(kneserney_perplexities)/len(kneserney_perplexities))) + '\\n' + testing_output\n",
        "file1=open(\"2019111031-LM3-test-perplexity.txt\", \"w\")\n",
        "file1.write(testing_output)\n",
        "file1.close()\n",
        "\n",
        "print(\"Testing sentences done\")\n",
        "\n",
        "# medical train_perplexities\n",
        "kneserney_perplexities = []\n",
        "training_output=\"\"\n",
        "for j, line in enumerate(medical.train_sentences):\n",
        "  words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "  words += Tokenizer(line)\n",
        "  words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "  prob = 0\n",
        "\n",
        "  if j%2000 == 0:\n",
        "    print(j, line)\n",
        "\n",
        "  size = len(words)\n",
        "  for i, word in enumerate(words):\n",
        "    if i < size-3:\n",
        "      kn = KneserNey(medical, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "      log_kn = math.log(kn)\n",
        "      prob += log_kn\n",
        "  final = math.e**(-prob/size)\n",
        "  training_output=training_output+((line.strip())+\"\\t\"+str(final))+\"\\n\"\n",
        "  kneserney_perplexities.append(final)\n",
        "\n",
        "training_output = str((sum(kneserney_perplexities)/len(kneserney_perplexities))) + '\\n' + training_output\n",
        "file1=open(\"2019111031-LM3-train-perplexity.txt\", \"w\")\n",
        "file1.write(training_output)\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sJoAkhu3QGU",
        "outputId": "2d826171-8ff1-4d0e-9f50-a1dfc6034c3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 That is precisely the time when you may, if you wish, raise this question, i.e. on Thursday prior to the start of the presentation of the report.\n",
            "\n",
            "100 In conclusion, the document is not particularly satisfactory, and in some aspects is disappointing.\n",
            "\n",
            "200 The centralised ex ante approval system was doubtlessly originally designed to ensure carefulness but, over the years, it has had the perverse effect of reducing the extent to which managers feel responsible for their decisions.\n",
            "\n",
            "300 Portugal should not take a leaf out its predecessor' s book.\n",
            "\n",
            "400 This is why it is useful for the European Union to continue to support the Member States and, if necessary, give additional support under URBAN.\n",
            "\n",
            "500 The proposed amendment is technical and cannot be interpreted as having as its primary objective the protection of public health.\n",
            "\n",
            "600 In order to combat distortion of competition and prevent dumping, the Commission has, in my opinion, developed a well-balanced approach which steers a midcourse between harmonisation and non-intervention.\n",
            "\n",
            "700 We provide partnership funding and there are cooperation agreements in place, in order to help Armenia, and still the aid cannot come into play because Turkey has imposed an economic blockade, and so many things cannot get through to the Armenians at all.\n",
            "\n",
            "800 As far as criminal sanctions are concerned, measures are needed to guarantee protection for the euro from the outset and this is the purpose of the draft framework decision on increasing protection by criminal sanctions against counterfeiting presented by the European Parliament.\n",
            "\n",
            "900 The first one concerns the timescale of these important reforms.\n",
            "\n",
            "Testing sentences done\n",
            "0 Resumption of the session\n",
            "\n",
            "2000 These guarantee - making no claim to be exhaustive - a minimum of 3 things.\n",
            "\n",
            "4000 You know that you will always have our support here in Parliament, and that includes your proposal, and we shall be behind you when the time comes to finance it.\n",
            "\n",
            "6000 The first is that greater resources should be given to judges of first instance, and that they should be given one more référendaire.\n",
            "\n",
            "8000 It would be even better if they were involved to a greater extent in the processes of framing and implementing the programmes concerned, otherwise it will be difficult to determine and defend their role in relation to the local and national authorities and even the citizens.\n",
            "\n",
            "10000 Therefore, our task is to make the requirements in the directive tighter and more concrete.\n",
            "\n",
            "12000 Certain politically important aspects of the new partnership agreement will have contributed to these.\n",
            "\n",
            "14000 We, the social democrats, would urge the opposition to back these ideas.\n",
            "\n",
            "16000 An area of special concern is the harassment of journalists, including temporary detention of members of the so-called independent press.\n",
            "\n",
            "18000 We know from our own experience that freedom and the ability to decide are not given freely but must be fought for.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file = \"./drive/MyDrive/NLP/europarl-corpus.txt\"\n",
        "\n",
        "with open(file) as f:\n",
        "  europarl = f.readlines()\n",
        "\n",
        "europarl = Corpus(europarl)\n",
        "europarl.process()\n",
        "\n",
        "# europarl test_perplexities\n",
        "wittenbell_perplexities = []\n",
        "testing_output=\"\"\n",
        "for j, line in enumerate(europarl.test_sentences):\n",
        "  words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "  words += Tokenizer(line)\n",
        "  words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "  prob = 0\n",
        "\n",
        "  if j%100 == 0:\n",
        "    print(j, line)\n",
        "\n",
        "  size = len(words)\n",
        "  for i, word in enumerate(words):\n",
        "    if i < size-3:\n",
        "      kn = Wittenbell(europarl, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "      log_kn = math.log(kn)\n",
        "      prob += log_kn\n",
        "  final = math.e**(-prob/size)\n",
        "  testing_output=testing_output+((line.strip())+\"\\t\"+str(final))+\"\\n\"\n",
        "  wittenbell_perplexities.append(final)\n",
        "\n",
        "testing_output = str((sum(wittenbell_perplexities)/len(wittenbell_perplexities))) + '\\n' + testing_output\n",
        "file1=open(\"2019111031-LM2-test-perplexity.txt\", \"w\")\n",
        "file1.write(testing_output)\n",
        "file1.close()\n",
        "\n",
        "print(\"Testing sentences done\")\n",
        "\n",
        "# europarl train_perplexities\n",
        "wittenbell_perplexities = []\n",
        "training_output=\"\"\n",
        "for j, line in enumerate(europarl.train_sentences):\n",
        "  words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "  words += Tokenizer(line)\n",
        "  words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "  prob = 0\n",
        "\n",
        "  if j%2000 == 0:\n",
        "    print(j, line)\n",
        "\n",
        "  size = len(words)\n",
        "  for i, word in enumerate(words):\n",
        "    if i < size-3:\n",
        "      kn = Wittenbell(europarl, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "      log_kn = math.log(kn)\n",
        "      prob += log_kn\n",
        "  final = math.e**(-prob/size)\n",
        "  training_output=training_output+((line.strip())+\"\\t\"+str(final))+\"\\n\"\n",
        "  wittenbell_perplexities.append(final)\n",
        "\n",
        "training_output = str((sum(wittenbell_perplexities)/len(wittenbell_perplexities))) + '\\n' + training_output\n",
        "file1=open(\"2019111031-LM2-train-perplexity.txt\", \"w\")\n",
        "file1.write(training_output)\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wJHVwhZ0v2g",
        "outputId": "cd39d7a2-2e64-48ce-991e-0b59e7e723ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 On the other hand, GCS induce the reductive activity of 11-HSD and, therefore, they increase their own activation by positive feedback.\n",
            "\n",
            "100 Abdominal aortic cross-clamping changes circulation, nutritional supply of the lower extremities and thus V.O2 and V.CO2.\n",
            "\n",
            "200 Cleft lip and palates (CLP) are caused by a variety of factors.\n",
            "\n",
            "300 In view of the poor visual and ocular prognosis in severe retinal vessel occlusion, controlled clinical trials are needed to determine the benefit of thrombolysis in the management of this disease.\n",
            "\n",
            "400 The average time from loss of consciousness to tracheal intubation was 164 +/- 8 s. Conclusions.\n",
            "\n",
            "500 Typ II: The leading thoracic injury is a lung contusion which occurs in up to 50 % of the cases.\n",
            "\n",
            "600 Magnetic resonance tomography (MRT) has become the most important method in the workup of infantile cerebral complications after primary sonography.\n",
            "\n",
            "700 Delayed diagnoses were defined as the difference between the ISS at discharge and the ISS at completion of diagnostics in the emergency department; this criterion was met best by clinic A with an ISS difference of two patients compared to five in clinic B and four in clinic C.\n",
            "\n",
            "800 The M/D group revealed in the first week a significantly faster reduction of the PASI-score (5,3) than in the D/D group (PASI 13,22).\n",
            "\n",
            "900 There are two types of diabetes mellitus.\n",
            "\n",
            "Testing sentences done\n",
            "0 Although there are a few isolated reports in the literature suggesting that sugar beet pollen is highly antigenic, hypersensitivity to components of sugar beet is not a common disease.\n",
            "\n",
            "2000 This development implies new approaches to the psychoanalytic process and technique, and especially to interpretation.\n",
            "\n",
            "4000 Nevertheless, efficacy of buprenorphine in maintenance could be demonstrated in the remaining subjects, and further studies with higher daily doses and a higher number of subjects have to be performed.\n",
            "\n",
            "6000 With the increasing number of patients surviving after therapeutic intervention for congenital heart disease (CHD), accurate and frequent follow-up of their morphologic and functional cardiovascular status is required, preferably with a noninvasive imaging technique.\n",
            "\n",
            "8000 Therapy consisted of local excision of the bronchus and postoperative radiotherapy.\n",
            "\n",
            "10000 A verrucous carcinoma of the urinary bladder (pT4N0M0, G1) developed in a 66-year-old woman who had been suffering from interstitial cystitis with Hunner's ulcer for 10 years.\n",
            "\n",
            "12000 However, the exact role and precise sources of NO under physiological and pathophysiological conditions within the airways remain to be defined.\n",
            "\n",
            "14000 In particular, post-traumatic CPR is associated with an extremely poor outcome, leading to the issue of futility.\n",
            "\n",
            "16000 The EWL after lab band for 24 patients after 12 months was 47 (11-127) % and for 8 patients after 18 months 51 (28-139) %.\n",
            "\n",
            "18000 Following approval by the ethics commission, the endopass was implanted in five patients between June 1997 and March 1998.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "file = \"./drive/MyDrive/NLP/medical-corpus.txt\"\n",
        "\n",
        "with open(file) as f:\n",
        "  medical = f.readlines()\n",
        "\n",
        "medical = Corpus(medical)\n",
        "medical.process()\n",
        "\n",
        "# medical test_perplexities\n",
        "wittenbell_perplexities = []\n",
        "testing_output=\"\"\n",
        "for j, line in enumerate(medical.test_sentences):\n",
        "  words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "  words += Tokenizer(line)\n",
        "  words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "  prob = 0\n",
        "\n",
        "  if j%100 == 0:\n",
        "    print(j, line)\n",
        "\n",
        "  size = len(words)\n",
        "  for i, word in enumerate(words):\n",
        "    if i < size-3:\n",
        "      kn = Wittenbell(medical, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "      log_kn = math.log(kn)\n",
        "      prob += log_kn\n",
        "  final = math.e**(-prob/size)\n",
        "  testing_output=testing_output+((line.strip())+\"\\t\"+str(final))+\"\\n\"\n",
        "  wittenbell_perplexities.append(final)\n",
        "\n",
        "testing_output = str((sum(wittenbell_perplexities)/len(wittenbell_perplexities))) + '\\n' + testing_output\n",
        "file1=open(\"2019111031-LM4-test-perplexity.txt\", \"w\")\n",
        "file1.write(testing_output)\n",
        "file1.close()\n",
        "\n",
        "print(\"Testing sentences done\")\n",
        "\n",
        "# medical train_perplexities\n",
        "wittenbell_perplexities = []\n",
        "training_output=\"\"\n",
        "for j, line in enumerate(medical.train_sentences):\n",
        "  words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "  words += Tokenizer(line)\n",
        "  words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "  prob = 0\n",
        "\n",
        "  if j%2000 == 0:\n",
        "    print(j, line)\n",
        "\n",
        "  size = len(words)\n",
        "  for i, word in enumerate(words):\n",
        "    if i < size-3:\n",
        "      kn = Wittenbell(medical, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "      log_kn = math.log(kn)\n",
        "      prob += log_kn\n",
        "  final = math.e**(-prob/size)\n",
        "  training_output=training_output+((line.strip())+\"\\t\"+str(final))+\"\\n\"\n",
        "  wittenbell_perplexities.append(final)\n",
        "\n",
        "training_output = str((sum(wittenbell_perplexities)/len(wittenbell_perplexities))) + '\\n' + training_output\n",
        "file1=open(\"2019111031-LM4-train-perplexity.txt\", \"w\")\n",
        "file1.write(training_output)\n",
        "file1.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qA_HkqL_5m9",
        "outputId": "194b98db-e7a9-44e0-8afa-d9cb92243dd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usage: <smoothing type> <path to corpus>\n",
            "w ./drive/MyDrive/NLP/medical-corpus.txt\n",
            "Enter sentence: I am a man\n",
            "\n",
            "Probability: 6.964944857213407e-166\n"
          ]
        }
      ],
      "source": [
        "inp = input(\"Usage: <smoothing type> <path to corpus>\\n\")\n",
        "inp = inp.split()\n",
        "\n",
        "if len(inp) != 2 or (inp[0]!='k' and inp[0]!='w') or os.path.exists(inp[1]) == False:\n",
        "  print(\"Error\")\n",
        "else:\n",
        "  with open(file) as f:\n",
        "    corpus = f.readlines()\n",
        "  corpus = Corpus(corpus)\n",
        "  corpus.process()\n",
        "\n",
        "  st = input(\"Enter sentence: \")\n",
        "  final_prob = 1\n",
        "  for j, line in enumerate(st):\n",
        "    words = [\"<s>\",\"<s>\",\"<s>\"]\n",
        "    words += Tokenizer(line)\n",
        "    words += [\"</s>\",\"</s>\",\"</s>\"]\n",
        "\n",
        "    size = len(words)\n",
        "    for i, word in enumerate(words):\n",
        "      if i < size-3:\n",
        "        if inp[0] == 'w':\n",
        "          p = Wittenbell(corpus, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "        else:\n",
        "          p = KneserNey(corpus, (words[i], words[i+1], words[i+2], words[i+3]))\n",
        "        final_prob *= p\n",
        "  print(f'\\nProbability: {final_prob}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "language_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
